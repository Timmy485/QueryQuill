import pandas as pd
import json
import replicate
import os

# Determine the path where this script is located
path = os.path.dirname(__file__)
json_path = path + '/config.json'

# Load configuration data from the JSON file
with open(json_path, 'r') as config_file:
    config_data = json.load(config_file)

# Extract the API token from the configuration data
api_key = config_data.get('REPLICATE_API_TOKEN')


def generate_direct_answer_with_llama(search_results, user_query, save_csv=True):
    """
    Generate a direct answer using the LLAMA model from Replicate.

    Args:
        search_results (list): List of search results, typically from Elasticsearch.
        user_query (str): The user's query/question.
        save_csv (bool): Flag indicating if the results should be saved to a CSV.

    Returns:
        str: The answer generated by the LLAMA model.
    """
    
    # Initialize the Replicate client using the provided API token
    rep = replicate.Client(api_token=api_key)
    passages = ''

    # If save_csv is True, read the questions and answers from a CSV and combine the passages
    if save_csv:
        df = pd.read_csv('../docs/questions_answers.csv', encoding='ISO-8859-1')
        passage_columns = [col for col in df.columns if "Passage" in col and "Metadata" not in col]
        combined_passages = df[passage_columns].apply(lambda row: ' '.join(row.dropna()), axis=1)
        passages = "".join(combined_passages)
    else:
        passages = [hit["_source"]["Passage"] for hit in search_results]

    # Create a prompt for the LLAMA model using the passages and user query
    prompt = f"""
        Use ONLY the following pieces of passages to answer the question at the end. 
        If you don't know the answer, just say that you don't know, don't try to make up an answer. 
        Use three sentences maximum and keep the answer as concise as possible. 
        Only answer from the passages
        Passages: {' '.join(passages)}
        Question: {user_query}
    """

    # Run the LLAMA model with the generated prompt
    output = rep.run(
        "meta/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1",
        input={"prompt": prompt},
        temperature=0.1,
        seed=0
    )

    # Extract the generated answer from the model's output
    out = "".join([item for item in output])

    # If save_csv is True, save the generated answer to a CSV
    if save_csv:
        df['Generative AI Answer'] = out
        df.to_csv('../docs/questions_answers.csv', index=False)
    
    return out
